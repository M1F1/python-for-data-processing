{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 12 - hypothesis testing\n",
    "\n",
    "\n",
    "<img src=\"img/12/significant.png\">\n",
    "\n",
    "## Data science is OSEMN\n",
    "According to a popular model, the elements of data science are\n",
    "\n",
    "* Obtaining data\n",
    "* Scrubbing data\n",
    "* Exploring data\n",
    "* Modeling data\n",
    "* Interpreting data\n",
    "\n",
    "We have looked at every step of this process! This lab we look more closely at something useful for interpreting data: hypothesis testing. The main point of the lab is to learn how to interpret p-values and what they **really** mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis testing\n",
    "\n",
    "Great sources of information: https://en.wikipedia.org/wiki/P-value, https://en.wikipedia.org/wiki/Statistical_hypothesis_testing, http://ipython-books.github.io/featured-07/\n",
    "\n",
    "Hypothesis testing is about.. testing hypothesis. Process has following steps:\n",
    "\n",
    "1. Writing down the hypotheses, notably the null hypothesis which is the opposite of the hypothesis we want to prove (with a certain degree of confidence).\n",
    "2. Computing a test statistics, a mathematical formula depending on the test type, the model, the hypotheses, and the data.\n",
    "3. Using the computed value to accept the hypothesis, reject it, or fail to conclude."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "Is the coin fair? \n",
    "1. Null hypothesis: coin is fair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "import scipy.special as sp\n",
    "n = 100  # number of coin flips\n",
    "h = 61  # number of heads\n",
    "q = .5  # null-hypothesis of fair coin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute z-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.1999999999999997"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbar = float(h)/n\n",
    "z = (xbar - q) * np.sqrt(n / (q*(1-q))); z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02780689502699718"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pval = 2 * (1 - st.norm.cdf(z))\n",
    "pval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This p-value is less than 0.05, so we reject the null hypothesis and conclude that the coin is probably not fair.\n",
    "\n",
    "pval = $P(n_{heads} = 61 | \\mbox{is fair}) = 0.0278$\n",
    "\n",
    "<img src=\"http://ipython-books.github.io/images/gaussian.png\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting p-values\n",
    "\n",
    "Sources: http://www.johndcook.com/blog/2008/11/18/five-criticisms-of-significance-testing/, http://blog.minitab.com/blog/adventures-in-statistics/how-to-correctly-interpret-p-values\n",
    "\n",
    "P values address only one question: how likely are your data, assuming a true null hypothesis? It does not measure support for the alternative hypothesis. \n",
    "\n",
    "Low P value indicates that your data are unlikely assuming a true null, it can’t evaluate which of two competing cases is more likely:\n",
    "\n",
    "* The null is true but your sample was unusual.\n",
    "* The null is false.\n",
    "\n",
    "This is quite tricky, but we have to use Bayes rule to ask $P(H_0 | D)$ (p value is $P(D | H_0)$). Usually:\n",
    "\n",
    "<img width=600 src=\"img/12/img2.png\">\n",
    "\n",
    "So please be cautious :)\n",
    "\n",
    "<img width=300 src=\"img/12/img1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1,  1 point\n",
    "\n",
    "Read http://www.nature.com/news/scientific-method-statistical-errors-1.14700#/b1 \n",
    "    and explain in 2 sentences why p-values are dangerous and often lead to \n",
    "    incorrect results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple testing\n",
    "\n",
    "One of the problems with p-values is that if you perform enough tests, you will definitely reject the null, unless\n",
    "you correct for multiple testing. Simplest correction is Bonferroni, which simply divides threshold p-value by number of tests performed (which follows directly from union bound)\n",
    "\n",
    "<img src=\"https://imgs.xkcd.com/comics/p_values.png\">\n",
    "\n",
    "## Exercise 2, 3 points\n",
    "\n",
    "Read http://multithreaded.stitchfix.com/blog/2015/10/15/multiple-hypothesis-testing/ and:\n",
    "\n",
    "* Replicate experiment (as python code, remember to include plots) in \"Simple example: Difference between p-value and being “right”\" section\n",
    "* Create artificial example that shows that correcting for multiple tests is necessary (you can get inspiration from ipython notebook that is provided with the blog post or from xkcd strap that is linked with this notebook)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
